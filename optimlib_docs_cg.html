<!DOCTYPE html>
<html lang="en">

<head>

    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    
    <meta name="description" content="Bayesian Macroeconometrics in R (BMR)">
    <meta name="author" content="Keith O'Hara">

    <meta name="keywords" content="Bayesian, Macroeconometrics, BMR, Economics, PhD student, NYU, New York University, Econometrics, R, C++, Cpp, Research" />

    <link rel="shortcut icon" type="image/x-icon" href="siteicon.ico">

    <title>OptimLib: CG</title>

    <!-- Bootstrap Core CSS -->
    <link href="css/bootstrap.min.css" rel="stylesheet">

    <!-- Custom CSS -->
    <link href="css/modern-business.css" rel="stylesheet">

    <!-- Custom Fonts -->
    <link href="font-awesome/css/font-awesome.min.css" rel="stylesheet" type="text/css">

    <!-- Syntax Highlighter -->
    <script type="text/javascript" src="js/syntaxhighlighter.js"></script>
    <link type="text/css" rel="stylesheet" href="css/swift_theme.css">

    <style>
    h3:target { padding-top: 60px; margin-top: -60px;}
    </style>

    <script>
    (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
    (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
    m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
    })(window,document,'script','https://www.google-analytics.com/analytics.js','ga');
    
    ga('create', 'UA-93902857-1', 'auto');
    ga('send', 'pageview');

    </script>

    <!-- MathJax -->
    <script type="text/x-mathjax-config">
        MathJax.Hub.Config({tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]}});
    </script>
    <script type="text/javascript" async
        src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS_CHTML">
    </script>

    <script async defer src="https://buttons.github.io/buttons.js"></script>

    <script src="js/jquery.js"></script>
    <script>
        $(function(){
            $("#mynavbar").load("navbar.html")
        });
    </script>

</head>

<style>
pre {
    display: inline-block;
}
</style>

<body>

    <!-- Navigation -->
    <div id="mynavbar"></div>

    <!-- Page Content -->
    <div class="container">

        <!-- Page Heading/Breadcrumbs -->
        <div class="row">
            <div class="col-lg-12">
                <ol class="breadcrumb">
                    <li><a href="http://www.kthohr.com">Home</a></li>
                    <li><a href="optimlib">OptimLib</a></li>
                    <li class="dropdown">
                        <a href="#" class="dropdown-toggle" data-toggle="dropdown">Algorithms<b class="caret"></b></a>
                        <ul class="dropdown-menu">
                            <li><a href="optimlib_docs_bfgs.html">BFGS</a></li>
                            <li><a href="optimlib_docs_broyden.html">Broyden</a></li>
                            <li class="active"><a href="optimlib_docs_cg.html">Conjugate Gradient</a></li>
                            <li><a href="optimlib_docs_de.html">Differential Evolution</a></li>
                            <li><a href="optimlib_docs_nm.html">Nelder-Mead</a></li>
                            <li class="divider"></li>
                            <li><a href="optimlib_docs_sumt.html">SUMT</a></li>
                        </ul>
                    </li>
                    <li class="dropdown">
                        <a href="#" class="dropdown-toggle" data-toggle="dropdown">Misc<b class="caret"></b></a>
                        <ul class="dropdown-menu">
                            <li><a href="optimlib_docs_settings.html">opt_settings</a></li>
                        </ul>
                    </li>
                </ol>
            </div>
        </div>
        <!-- /.row -->

<h3 style="text-align: left;"><strong style="font-size: 120%;">OptimLib: Nonlinear Conjugate Gradient Method</strong></h3>

<hr>
<!--<p> &nbsp </p>-->

<p>The Nonlinear Conjugate Gradient (CG) method generalizes its <a href="https://en.wikipedia.org/wiki/Conjugate_gradient_method">linear namesake</a> to a broader class of convex optimization problems.</p>

<p><strong>Function definition:</strong></p>
<pre class="brush: cpp;">
// internal
bool cg_int(arma::vec& init_out_vals, std::function&lt;double (const arma::vec& vals_inp, arma::vec* grad_out, void* opt_data)&gt; opt_objfn, void* opt_data, double* value_out, optim_opt_settings* opt_params);

// wrappers
bool cg(arma::vec& init_out_vals, std::function&lt;double (const arma::vec& vals_inp, arma::vec* grad_out, void* opt_data)&gt; opt_objfn, void* opt_data);
bool cg(arma::vec& init_out_vals, std::function&lt;double (const arma::vec& vals_inp, arma::vec* grad_out, void* opt_data)&gt; opt_objfn, void* opt_data, optim_opt_settings& opt_params);
bool cg(arma::vec& init_out_vals, std::function&lt;double (const arma::vec& vals_inp, arma::vec* grad_out, void* opt_data)&gt; opt_objfn, void* opt_data, double& value_out);
bool cg(arma::vec& init_out_vals, std::function&lt;double (const arma::vec& vals_inp, arma::vec* grad_out, void* opt_data)&gt; opt_objfn, void* opt_data, double& value_out, optim_opt_settings& opt_params);
</pre>
<p><strong>Function arguments:</strong></p>
<ul>
    <li><code>init_out_vals</code> a column vector of initial values; will contain the final values.</li>
    <li><code>opt_objfn</code> the function to be minimized, taking three arguments: 
        <ul>
            <li><code>vals_inp</code> a vector of inputs;</li>
            <li><code>grad_out</code> a vector to store the gradient values; and</li>
            <li><code>opt_data</code> additional parameters passed to the function.</li>
        </ul>
    <li><code>opt_data</code> additional parameters passed to the function.</li>
    <li><code>opt_params</code> parameters controlling the optimization routine; see below.</li>
</ul>

<p><strong>Optimization control parameters:</strong></p>
<ul>
    <li><code>double err_tol</code> the value controlling how small $\| \nabla f \|$ should be before 'convergence' is declared.</li>
    <li><code>int iter_max</code> the maximum number of iterations/updates before the algorithm exits.</li>
    <li><code>int method_cg</code> which updating formula to use; see the details section below.</li>
    <li><code>double cg_restart_threshold</code> the value $\nu$ in the details section below.</li>
</ul>

<hr>

<h3 style="text-align: left;"><strong style="font-size: 100%;">Details:</strong></h3>

<p>Let $x_i$ denote the values at stage $i$ of the algorithm. The Nonlinear Conjugate Gradient updating rule is as follows.</p>
$$d_i = - \nabla f_{i} + \beta_i \times d_{i-1}$$
$$x_{i+1} = x_i + \alpha_{i} d_{i}$$
<p>where $\beta$ is based on one of the updating rules below, and $\alpha_i$ is found via line search:</p>
$$\alpha_i = \arg \min_\alpha f(x_i + \alpha d_i)$$
<p>The Nonlinear CG method is known to perform better with periodic restarting (setting $\beta = 0$). In OptimLib, this occurs when</p>
$$\dfrac{|\nabla f_i \cdot \nabla f_{i-1}|}{\nabla f_{i} \cdot \nabla f_{i}} > \nu$$

<p>The algorithm stops when $\| \nabla f \|$ is less than err_tol, or the total number of iterations exceeds a desired (or default) value.</p>

<hr>
<h3 style="text-align: left;"><strong style="font-size: 80%;">Updating Formula:</strong></h3>
<ul>
    <li> Fletcherâ€“Reeves (<code>method_cg = 1</code>):</li>
        $$\beta_{\text{FR}} = \dfrac{\nabla f_i \cdot \nabla f_i}{\nabla f_{i-1} \cdot \nabla f_{i-1}}$$
    <li> Polak-Ribiere (PR) '+' (<code>method_cg = 2</code>):</li>
        $$\beta_{\text{PR+}} = \max \left\{ 0 , \dfrac{\nabla f_i \cdot (\nabla f_i - \nabla f_{i-1})}{\nabla f_{i-1} \cdot \nabla f_{i-1}} \right\}$$
    <li> FR-PR Hybrid: (<code>method_cg = 3</code>):</li>
    <ul>
        <li>If $\beta_{\text{PR}} < - \beta_{\text{FR}}$, set $\beta = - \beta_{\text{FR}}$</li>
        <li>If $|\beta_{\text{PR}}| < \beta_{\text{FR}}$, set $\beta = \beta_{\text{PR}}$</li>
        <li>If $\beta_{\text{PR}} > \beta_{\text{FR}}$, set $\beta = \beta_{\text{FR}}$</li>
    </ul>
    <li> Hestenes-Stiefel: (<code>method_cg = 4</code>):</li>
        $$\beta_{\text{HS}} = \dfrac{\nabla f_i \cdot (\nabla f_i - \nabla f_{i-1})}{(\nabla f_{i} - \nabla f_{i-1}) \cdot d_i}$$
    <li> Dai-Yuan: (<code>method_cg = 5</code>):</li>
        $$\beta_{\text{DY}} = \dfrac{\nabla f_i \cdot \nabla f_i}{(\nabla f_{i} - \nabla f_{i-1}) \cdot d_i}$$
    <li> Hager-Zhang: (<code>method_cg = 6</code>):</li>
        $$\beta_{\text{HZ}} = \left( y - 2 \times \dfrac{y \cdot y}{y \cdot d} \times d \right) \cdot \dfrac{\nabla f_i}{y \cdot d}, \ \ \ y := \nabla f_i - \nabla f_{i-1}$$
</ul>

        <hr>

        <!-- Footer -->
        <footer>
            <div class="row">
                <div class="col-lg-12">
                    <p>Copyright &copy; Keith O'Hara 2017</p>
                </div>
            </div>
        </footer>

    </div>
    <!-- /.container -->

    <!-- jQuery -->
    <!--<script src="js/jquery.js"></script>-->

    <!-- Bootstrap Core JavaScript -->
    <script src="js/bootstrap.min.js"></script>

</body>

</html>
